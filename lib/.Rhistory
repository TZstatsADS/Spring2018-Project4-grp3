mode <- rep(0, p)
theta <- rep(0, p)
loss <- rep(0, p)
# theta[j_star] is the global extremum of a cumulative sum
for(j in 1:p){
##j=1
index <- order(X[,j])
x_j <- X[index, j]
rev_x_j <- X[rev(index), j]
w_cum <- cumsum(w[index] * y[index])
w_cum[rev(duplicated(rev_x_j) == 1)] <- NA
m <- max(abs(w_cum), na.rm = T)
maxIndex <- min(which(abs(w_cum) == m))
mode[j] <- (w_cum[maxIndex] < 0) * 2 - 1
theta[j] <- x_j[maxIndex]
c <- ((X[,j] > theta[j]) * 2 - 1) * mode[j]
loss[j] <- w %*% (c != y)
}
m <- min(loss)
j_star <- min(which(loss == m))
pars <- list(j = j_star, theta = theta[j_star], mode = mode[j_star])
return(pars)
}
classify <- function(X, pars){
label <- (2*(X[, pars$j] > pars$theta) - 1) * pars$mode
return(label)
}
set.seed(123)
B_max <- 60
nCV <- 5
zip.3<-read.table("train_3.txt", header=FALSE, sep=",")
zip.3<-as.matrix(zip.3)
zip.3<-cbind(y = rep(1, nrow(zip.3)), zip.3)
zip.8<-read.table("train_8.txt", header=FALSE, sep=",")
zip.8<-as.matrix(zip.8)
zip.8<-cbind(y = rep(-1, nrow(zip.8)), zip.8)
train.set<-data.frame(rbind(zip.3, zip.8))
test <- read.table("zip_test.txt", header=FALSE)
#test <- data.frame(as.matrix(test))
test <- test[test$V1 %in% c(3,8),]
test$V1 <- ifelse(test$V1==3, 1, 8)
train.set <- rbind(train.set, test)
knitr::opts_chunk$set(echo = TRUE)
adaBoost <- function(X, y, B){
n <- dim(X)[1] # number of obs.
w <- rep(1/n, n) # initial weights
alpha <- rep(0, B) # initial alpha
allPars <- rep(list(list()), B) # list of parameters at each iteration
for (b in 1:B) {
# train weak lerner on the weighted training data
pars <- train(X, w, y)
allPars[[b]] <- pars
# compute error
label <- classify(X, pars)
e <- ((w %*% (y != label))/ sum(w)) [1]
# compute voting weights
alpha[b] <- log((1-e)/e)
# recompute weights
w <- w*exp(alpha[b]*(y != label))
}
return(list(allPars = allPars, alpha = alpha))
}
agg_class <- function(X, alpha, allPars){
n <- dim(X)[1]
B <- length(alpha)
label_mat <- matrix(0, nrow = n, ncol = B)
c_hat <- rep(0, n)
# store the result of B iteration in label_mat
for (b in 1:B) {
label_mat[,b] <- classify(X, allPars[[b]])
}
# for each observation i, calculate the predicted label
for (i in 1:n) {
c_hat[i] <- sign(sum(alpha * label_mat[i,]))
}
return(c_hat)
}
train <- function(X, w, y){
n <- dim(X)[1]
p <- dim(X)[2]
mode <- rep(0, p)
theta <- rep(0, p)
loss <- rep(0, p)
# theta[j_star] is the global extremum of a cumulative sum
for(j in 1:p){
##j=1
index <- order(X[,j])
x_j <- X[index, j]
rev_x_j <- X[rev(index), j]
w_cum <- cumsum(w[index] * y[index])
w_cum[rev(duplicated(rev_x_j) == 1)] <- NA
m <- max(abs(w_cum), na.rm = T)
maxIndex <- min(which(abs(w_cum) == m))
mode[j] <- (w_cum[maxIndex] < 0) * 2 - 1
theta[j] <- x_j[maxIndex]
c <- ((X[,j] > theta[j]) * 2 - 1) * mode[j]
loss[j] <- w %*% (c != y)
}
m <- min(loss)
j_star <- min(which(loss == m))
pars <- list(j = j_star, theta = theta[j_star], mode = mode[j_star])
return(pars)
}
classify <- function(X, pars){
label <- (2*(X[, pars$j] > pars$theta) - 1) * pars$mode
return(label)
}
set.seed(123)
B_max <- 60
nCV <- 5
zip.3<-read.table("train_3.txt", header=FALSE, sep=",")
zip.3<-as.matrix(zip.3)
zip.3<-cbind(y = rep(1, nrow(zip.3)), zip.3)
zip.8<-read.table("train_8.txt", header=FALSE, sep=",")
zip.8<-as.matrix(zip.8)
zip.8<-cbind(y = rep(-1, nrow(zip.8)), zip.8)
train.set<-data.frame(rbind(zip.3, zip.8))
test <- read.table("zip_test.txt", header=FALSE)
#test <- data.frame(as.matrix(test))
test <- test[test$V1 %in% c(3,8),]
test$V1 <- ifelse(test$V1==3, 1, 8)
colnames(test) <- colnames(train.set)
train.set <- rbind(train.set, test)
X <- train.set[,-1]
y <- train.set[,1] # label 1 means 3, -1 means 8
n <- dim(X)[1]
testError <- matrix(0, nrow = B_max, ncol = nCV)
trainError <- matrix(0, nrow = B_max, ncol = nCV)
n <- length(y)
n.fold <- floor(n/nCV)
s <- sample(rep(1:nCV, c(rep(n.fold, nCV-1), n-(nCV-1)*n.fold)))
for(i in 1:nCV){
ada <- adaBoost(X[s != i,], y[s != i], B_max)
allPars <- ada$allPars
alpha <- ada$alpha
for (B in 1:B_max) {
c_hat_test <- agg_class(X[s == i,], alpha[1:B], allPars[1:B])
c_hat_train <- agg_class(X[s != i,], alpha[1:B], allPars[1:B])
testError[B, i] <- mean(y[s == i] != c_hat_test)
trainError[B, i] <- mean(y[s != i] != c_hat_train)
}
}
matplot(trainError, type = "l", lty = 1:nCV, main = "training error", xlab = "number of base classifers", ylab = "error rate", ylim = c(0, 0.16))
matplot(testError, type = "l", lty = 1:nCV, main = "test error", xlab = "number of base classifers", ylab = "error rate", ylim = c(0, 0.16))
testError.vec <- apply(testError, 1, mean)
testError.vec
zip.3<-read.table("train_3.txt", header=FALSE, sep=",")
zip.3<-as.matrix(zip.3)
zip.3<-cbind(y = rep(1, nrow(zip.3)), zip.3)
zip.8<-read.table("train_8.txt", header=FALSE, sep=",")
zip.8<-as.matrix(zip.8)
zip.8<-cbind(y = rep(-1, nrow(zip.8)), zip.8)
train.set<-data.frame(rbind(zip.3, zip.8))
test <- read.table("zip_test.txt", header=FALSE)
#test <- data.frame(as.matrix(test))
test <- test[test$V1 %in% c(3,8),]
test$V1 <- ifelse(test$V1==3, 1, 8)
colnames(test) <- colnames(train.set)
train.set <- rbind(train.set, test)
dim(train.set)
train.set[1:5,1:5]
knitr::opts_chunk$set(echo = TRUE)
adaBoost <- function(X, y, B){
n <- dim(X)[1] # number of obs.
w <- rep(1/n, n) # initial weights
alpha <- rep(0, B) # initial alpha
allPars <- rep(list(list()), B) # list of parameters at each iteration
for (b in 1:B) {
# train weak lerner on the weighted training data
pars <- train(X, w, y)
allPars[[b]] <- pars
# compute error
label <- classify(X, pars)
e <- ((w %*% (y != label))/ sum(w)) [1]
# compute voting weights
alpha[b] <- log((1-e)/e)
# recompute weights
w <- w*exp(alpha[b]*(y != label))
}
return(list(allPars = allPars, alpha = alpha))
}
agg_class <- function(X, alpha, allPars){
n <- dim(X)[1]
B <- length(alpha)
label_mat <- matrix(0, nrow = n, ncol = B)
c_hat <- rep(0, n)
# store the result of B iteration in label_mat
for (b in 1:B) {
label_mat[,b] <- classify(X, allPars[[b]])
}
# for each observation i, calculate the predicted label
for (i in 1:n) {
c_hat[i] <- sign(sum(alpha * label_mat[i,]))
}
return(c_hat)
}
train <- function(X, w, y){
n <- dim(X)[1]
p <- dim(X)[2]
mode <- rep(0, p)
theta <- rep(0, p)
loss <- rep(0, p)
# theta[j_star] is the global extremum of a cumulative sum
for(j in 1:p){
##j=1
index <- order(X[,j])
x_j <- X[index, j]
rev_x_j <- X[rev(index), j]
w_cum <- cumsum(w[index] * y[index])
w_cum[rev(duplicated(rev_x_j) == 1)] <- NA
m <- max(abs(w_cum), na.rm = T)
maxIndex <- min(which(abs(w_cum) == m))
mode[j] <- (w_cum[maxIndex] < 0) * 2 - 1
theta[j] <- x_j[maxIndex]
c <- ((X[,j] > theta[j]) * 2 - 1) * mode[j]
loss[j] <- w %*% (c != y)
}
m <- min(loss)
j_star <- min(which(loss == m))
pars <- list(j = j_star, theta = theta[j_star], mode = mode[j_star])
return(pars)
}
classify <- function(X, pars){
label <- (2*(X[, pars$j] > pars$theta) - 1) * pars$mode
return(label)
}
set.seed(123)
B_max <- 60
nCV <- 5
zip.3<-read.table("train_3.txt", header=FALSE, sep=",")
zip.3<-as.matrix(zip.3)
zip.3<-cbind(y = rep(1, nrow(zip.3)), zip.3)
zip.8<-read.table("train_8.txt", header=FALSE, sep=",")
zip.8<-as.matrix(zip.8)
zip.8<-cbind(y = rep(-1, nrow(zip.8)), zip.8)
train.set<-data.frame(rbind(zip.3, zip.8))
# test <- read.table("zip_test.txt", header=FALSE)
# #test <- data.frame(as.matrix(test))
# test <- test[test$V1 %in% c(3,8),]
# test$V1 <- ifelse(test$V1==3, 1, 8)
# colnames(test) <- colnames(train.set)
# train.set <- rbind(train.set, test)
X <- train.set[,-1]
y <- train.set[,1] # label 1 means 3, -1 means 8
n <- dim(X)[1]
testError <- matrix(0, nrow = B_max, ncol = nCV)
trainError <- matrix(0, nrow = B_max, ncol = nCV)
n <- length(y)
n.fold <- floor(n/nCV)
s <- sample(rep(1:nCV, c(rep(n.fold, nCV-1), n-(nCV-1)*n.fold)))
for(i in 1:nCV){
ada <- adaBoost(X[s != i,], y[s != i], B_max)
allPars <- ada$allPars
alpha <- ada$alpha
for (B in 1:B_max) {
c_hat_test <- agg_class(X[s == i,], alpha[1:B], allPars[1:B])
c_hat_train <- agg_class(X[s != i,], alpha[1:B], allPars[1:B])
testError[B, i] <- mean(y[s == i] != c_hat_test)
trainError[B, i] <- mean(y[s != i] != c_hat_train)
}
}
# testError.vec <- apply(testError, 1, mean)
matplot(trainError, type = "l", lty = 1:nCV, main = "training error", xlab = "number of base classifers", ylab = "error rate", ylim = c(0, 0.16))
matplot(testError, type = "l", lty = 1:nCV, main = "test error", xlab = "number of base classifers", ylab = "error rate", ylim = c(0, 0.16))
test <- read.table("zip_test.txt", header=FALSE)
#test <- data.frame(as.matrix(test))
test <- test[test$V1 %in% c(3,8),]
test$V1 <- ifelse(test$V1==3, 1, 8)
colnames(test) <- colnames(train.set)
test$V1
test <- read.table("zip_test.txt", header=FALSE)
head(test)
#test <- data.frame(as.matrix(test))
test <- test[test$V1 %in% c(3,8),]
head(test)
test$V1
test$V1 <- ifelse(test$V1==3, 1, -1)
test$V1
knitr::opts_chunk$set(echo = TRUE)
adaBoost <- function(X, y, B){
n <- dim(X)[1] # number of obs.
w <- rep(1/n, n) # initial weights
alpha <- rep(0, B) # initial alpha
allPars <- rep(list(list()), B) # list of parameters at each iteration
for (b in 1:B) {
# train weak lerner on the weighted training data
pars <- train(X, w, y)
allPars[[b]] <- pars
# compute error
label <- classify(X, pars)
e <- ((w %*% (y != label))/ sum(w)) [1]
# compute voting weights
alpha[b] <- log((1-e)/e)
# recompute weights
w <- w*exp(alpha[b]*(y != label))
}
return(list(allPars = allPars, alpha = alpha))
}
agg_class <- function(X, alpha, allPars){
n <- dim(X)[1]
B <- length(alpha)
label_mat <- matrix(0, nrow = n, ncol = B)
c_hat <- rep(0, n)
# store the result of B iteration in label_mat
for (b in 1:B) {
label_mat[,b] <- classify(X, allPars[[b]])
}
# for each observation i, calculate the predicted label
for (i in 1:n) {
c_hat[i] <- sign(sum(alpha * label_mat[i,]))
}
return(c_hat)
}
train <- function(X, w, y){
n <- dim(X)[1]
p <- dim(X)[2]
mode <- rep(0, p)
theta <- rep(0, p)
loss <- rep(0, p)
# theta[j_star] is the global extremum of a cumulative sum
for(j in 1:p){
##j=1
index <- order(X[,j])
x_j <- X[index, j]
rev_x_j <- X[rev(index), j]
w_cum <- cumsum(w[index] * y[index])
w_cum[rev(duplicated(rev_x_j) == 1)] <- NA
m <- max(abs(w_cum), na.rm = T)
maxIndex <- min(which(abs(w_cum) == m))
mode[j] <- (w_cum[maxIndex] < 0) * 2 - 1
theta[j] <- x_j[maxIndex]
c <- ((X[,j] > theta[j]) * 2 - 1) * mode[j]
loss[j] <- w %*% (c != y)
}
m <- min(loss)
j_star <- min(which(loss == m))
pars <- list(j = j_star, theta = theta[j_star], mode = mode[j_star])
return(pars)
}
classify <- function(X, pars){
label <- (2*(X[, pars$j] > pars$theta) - 1) * pars$mode
return(label)
}
set.seed(123)
B_max <- 60
nCV <- 5
zip.3<-read.table("train_3.txt", header=FALSE, sep=",")
zip.3<-as.matrix(zip.3)
zip.3<-cbind(y = rep(1, nrow(zip.3)), zip.3)
zip.8<-read.table("train_8.txt", header=FALSE, sep=",")
zip.8<-as.matrix(zip.8)
zip.8<-cbind(y = rep(-1, nrow(zip.8)), zip.8)
train.set<-data.frame(rbind(zip.3, zip.8))
test <- read.table("zip_test.txt", header=FALSE)
head(test)
#test <- data.frame(as.matrix(test))
test <- test[test$V1 %in% c(3,8),]
test$V1 <- ifelse(test$V1==3, 1, -1)
colnames(test) <- colnames(train.set)
train.set <- rbind(train.set, test)
X <- train.set[,-1]
y <- train.set[,1] # label 1 means 3, -1 means 8
n <- dim(X)[1]
testError <- matrix(0, nrow = B_max, ncol = nCV)
trainError <- matrix(0, nrow = B_max, ncol = nCV)
n <- length(y)
n.fold <- floor(n/nCV)
s <- sample(rep(1:nCV, c(rep(n.fold, nCV-1), n-(nCV-1)*n.fold)))
for(i in 1:nCV){
ada <- adaBoost(X[s != i,], y[s != i], B_max)
allPars <- ada$allPars
alpha <- ada$alpha
for (B in 1:B_max) {
c_hat_test <- agg_class(X[s == i,], alpha[1:B], allPars[1:B])
c_hat_train <- agg_class(X[s != i,], alpha[1:B], allPars[1:B])
testError[B, i] <- mean(y[s == i] != c_hat_test)
trainError[B, i] <- mean(y[s != i] != c_hat_train)
}
}
# testError.vec <- apply(testError, 1, mean)
matplot(trainError, type = "l", lty = 1:nCV, main = "training error", xlab = "number of base classifers", ylab = "error rate", ylim = c(0, 0.16))
matplot(testError, type = "l", lty = 1:nCV, main = "test error", xlab = "number of base classifers", ylab = "error rate", ylim = c(0, 0.16))
testError.vec <- apply(testError, 1, mean)
testError.vec
trainError.vec <- apply(trainError, 1, mean)
library(ggplot2)
ggplot(testError.vec)+
geom_line()
error <- c(testError.vec, trainError.vec)
rep(c("testError", "trainError"), each = 60)
testError.vec <- apply(testError, 1, mean)
trainError.vec <- apply(trainError, 1, mean)
error.vec <- c(testError.vec, trainError.vec)
error <- cbind(rep(c("testError", "trainError"), each = 60), error.vec)
error
error <- as.data.frame(cbind(rep(c("testError", "trainError"), each = 60), error.vec))
error
ggplot(error)+
geom_line(aes(x=1:120, y=error.vec, color=V1))
error <- cbind(rep(c("testError", "trainError"), each = 60), error.vec)
error
error <- cbind(rep(c("testError", "trainError"), each = 60), error.vec)
ggplot(error)+
geom_line(aes(x=1:120, y=error.vec, color=V1))
plot(testError.vec, type = "l", lty = 1:nCV, main = "training error", xlab = "number of base classifers", ylab = "error rate", ylim = c(0, 0.16))
plot(trainError.vec, type = "l", lty = 1:nCV, main = "test error", xlab = "number of base classifers", ylab = "error rate", ylim = c(0, 0.16))
plot(testError.vec, type = "l", lty = 1:nCV, main = "training error", xlab = "number of base classifers", ylab = "error rate", ylim = c(0, 0.15))
plot(trainError.vec, type = "l", lty = 1:nCV, main = "test error", xlab = "number of base classifers", ylab = "error rate", ylim = c(0, 0.15))
plot(testError.vec, type = "l", lty = 1:nCV, main = "training error", xlab = "number of base classifers", ylab = "error rate", ylim = c(0, 0.15))
lines(trainError.vec, type = "l", lty = 1:nCV, main = "test error", xlab = "number of base classifers", ylab = "error rate", ylim = c(0, 0.15))
plot(testError.vec, type = "l", lty = 1:nCV, main = "Error", xlab = "number of base classifers (B)", ylab = "error rate", ylim = c(0, 0.15), col = "red")
lines(trainError.vec, type = "l", lty = 1:nCV, main = "test error", xlab = "number of base classifers", ylab = "error rate", ylim = c(0, 0.15), col = "blue")
plot(testError.vec, type = "l", lty = 1:nCV, main = "Error", xlab = "number of base classifers (B)", ylab = "error rate", ylim = c(0, 0.15), col = "red")
lines(trainError.vec, type = "l", lty = 1:nCV, main = "test error", xlab = "number of base classifers", ylab = "error rate", ylim = c(0, 0.15), col = "blue")
legend(50, y=0.15, legend = c("train error", "test error"), col=c("red", "blue"), lty=1, cex=0.8)
plot(testError.vec, type = "l", main = "Error", xlab = "number of base classifers (B)", ylab = "error rate", ylim = c(0, 0.15), col = "red")
lines(trainError.vec, type = "l", ylim = c(0, 0.15), col = "blue")
legend(50, y=0.15, legend = c("test error", "train error"), col=c("red", "blue"), lty=1, cex=0.8)
knitr::opts_chunk$set(echo = TRUE)
review_dtm
knitr::opts_chunk$set(echo = TRUE)
# words <- colnames(dtm.mat)
library(tm)
library(SnowballC)
library(data.table)
text <- fread("sub.csv")
##text$text <- iconv(enc2utf8(text$text),sub="byte")
text$text <- iconv(text$text,"WINDOWS-1252","UTF-8")
x <- text$text
tagger <- rdr_model(language = "English", annotation = "POS")
library(RDRPOSTagger)
install.packages(RDRPOSTagger)
library(RDRPOSTagger)
install.packages(RDRPOSTagger)
install.packages("RDRPOSTagger")
library(RDRPOSTagger)
install.packages("RDRPOSTagger")
install.packages("RDRPOSTagger", repos = "http://www.datatailor.be/rcube", type = "source")library(RDRPOSTagger)
install.packages("RDRPOSTagger", repos = "http://www.datatailor.be/rcube", type = "source")library(RDRPOSTagger)
install.packages("RDRPOSTagger", repos = "http://www.datatailor.be/rcube", type = "source")
library(RDRPOSTagger)
library("RDRPOSTagger")
install.packages("RDRPOSTagger", repos = "http://www.datatailor.be/rcube", type = "source")
library("RDRPOSTagger")
install.packages("RDRPOSTagger", repos = "http://www.datatailor.be/rcube", type = "source")
install.packages(rJava)
install.packages("rJava")
install.packages("RDRPOSTagger", repos = "http://www.datatailor.be/rcube", type = "source")
install.packages("rJava")
library(rJava)
install.packages("RDRPOSTagger", repos = "http://www.datatailor.be/rcube", type = "source")
setwd("~/Documents/GitHub/Spring2018-Project4-grp3/lib")
knitr::opts_chunk$set(echo = TRUE)
train <- read.csv("../data/data_sample/eachmovie_sample/data_train.csv")
train <- read.csv("../data/data_sample/eachmovie_sample/data_train.csv")
test <- read.csv("../data/data_sample/eachmovie_sample/data_test.csv")
# recalcculate the score (if larger than 6 -> 1)
train$rescore <- ifelse(train$Score>=6, 1, 0)
# only keep the rows/observations that rescore == 1
train_adj = train[which(train$rescore ==1),]
# get movie and user id (unique)
Movies = sort(unique(train_adj$Movie))
Users = unique(train_adj$User)
Movies
Users
?upper.tri
User_Matrix <- matrix(1:6,2,3)
User_Matrix
User_Matrix[upper.tri(User_Matrix)] <- t(User_Matrix)[upper.tri(User_Matrix)]
User_Matrix
User_Matrix <- matrix(1:9,4,3)
User_Matrix
User_Matrix <- matrix(1:9,4,3)
User_Matrix <- matrix(1:9,3,3)
User_Matrix
upper.tri(User_Matrix)]
upper.tri(User_Matrix)
User_Matrix[upper.tri(User_Matrix)]
t(User_Matrix)[upper.tri(User_Matrix)]
t(User_Matrix)
?mapply
8/2/3
4/3
rep(1:3, 4)
match(1:3, 2:8)
match(2:3, 1:8)
match(2:3, -11:8)
a <- -11:8
a{14,15}
a[c(14,15)]
moviesj = c(2,3)
moviesk = c(3,4,5)
length(moviesj) + length(moviesk) -
length(unique(c(moviesj,moviesk)))
0.25 * length(moviesj)
length(moviesj) + length(moviesk) -
length(unique(c(moviesj,moviesk)))) * 0.8 / length(moviesk) / length(moviesj)
(length(moviesj) + length(moviesk) -
length(unique(c(moviesj,moviesk)))) * 0.8 / length(moviesk) / length(moviesj)
# find the index of moviesj (each length moviesk) in Movies_
m1s = match(sort(rep(moviesj,length(moviesk))),Movies_)
# moviesj = c(2,3)
# moviesk = c(3,4,5)
Movies_ = c(1:6)
# find the index of moviesj (each length moviesk) in Movies_
m1s = match(sort(rep(moviesj,length(moviesk))),Movies_)
m2s = match(rep(moviesk,length(moviesj)),Movies_)
m1s
m2s
mins = (m1s+m2s - sqrt((m1s-m2s)^2) )/2
maxs = (m1s+m2s + sqrt((m1s-m2s)^2) )/2
mins
